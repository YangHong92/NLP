{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer-Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wZvzxqEiJ8hh",
        "02HoZ6MkK_Vl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcdrR8-qzPsO",
        "outputId": "6b946aaf-0d2e-4d76-f59a-068f9fccb4fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import math\n",
        "import importlib\n",
        "import copy\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import spacy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pkuseg==0.0.25\n",
        "!pip3 install https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.0.0/zh_core_web_sm-3.0.0.tar.gz\n",
        "# then restart runtime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "59GSgJo_zWwF",
        "outputId": "3e8d351e-d3d3-4bd0-b23b-c00c413292b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pkuseg==0.0.25\n",
            "  Downloading pkuseg-0.0.25-cp37-cp37m-manylinux1_x86_64.whl (50.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 50.2 MB 204 kB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from pkuseg==0.0.25) (0.29.24)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pkuseg==0.0.25) (1.19.5)\n",
            "Installing collected packages: pkuseg\n",
            "Successfully installed pkuseg-0.0.25\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.0.0/zh_core_web_sm-3.0.0.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.0.0/zh_core_web_sm-3.0.0.tar.gz (49.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 49.6 MB 209 kB/s \n",
            "\u001b[?25hCollecting spacy<3.1.0,>=3.0.0\n",
            "  Downloading spacy-3.0.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 4.8 MB/s \n",
            "\u001b[?25hCollecting spacy-pkuseg<0.1.0,>=0.0.27\n",
            "  Downloading spacy_pkuseg-0.0.28-cp37-cp37m-manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 43.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (2.23.0)\n",
            "Collecting catalogue<2.1.0,>=2.0.4\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (21.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (0.8.2)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 42.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (3.0.6)\n",
            "Collecting thinc<8.1.0,>=8.0.3\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 45.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (4.62.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (1.0.6)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 47.4 MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.5\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (3.10.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (2.11.3)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: cython>=0.25 in /usr/local/lib/python3.7/dist-packages (from spacy-pkuseg<0.1.0,>=0.0.27->zh-core-web-sm==3.0.0) (0.29.24)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->zh-core-web-sm==3.0.0) (2.0.1)\n",
            "Building wheels for collected packages: zh-core-web-sm\n",
            "  Building wheel for zh-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zh-core-web-sm: filename=zh_core_web_sm-3.0.0-py3-none-any.whl size=49546698 sha256=6a436cd6d6df706f54dd8cc811a071fe76f80a2928a1cbd3f7ec86c0e120f0d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/aa/cf/352f6b353d6651057b80cd1a0b47d0a8fa4a4500b19435e96f\n",
            "Successfully built zh-core-web-sm\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-legacy, pathy, spacy-pkuseg, spacy, zh-core-web-sm\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 pathy-0.6.1 pydantic-1.8.2 spacy-3.0.7 spacy-legacy-3.0.8 spacy-pkuseg-0.0.28 srsly-2.4.2 thinc-8.0.13 typer-0.3.2 zh-core-web-sm-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "catalogue",
                  "spacy",
                  "srsly",
                  "thinc"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"zh_core_web_sm\")\n",
        "doc = nlp(\"你好，这里是中国。\")\n",
        "print(doc.text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kObB2zx5PSlz",
        "outputId": "e60eed8c-d411-4560-b03a-78b47446f6d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "你好，这里是中国。\n",
            "你好 VERB ROOT\n",
            "， PUNCT punct\n",
            "这里 PRON nsubj\n",
            "是 VERB cop\n",
            "中国 PROPN conj\n",
            "。 PUNCT punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Transformer\n",
        "\n",
        "![transformer](https://miro.medium.com/max/644/1*46c7LPV22532Svcewui37g.png)"
      ],
      "metadata": {
        "id": "_h83DocDAgco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Tokenize"
      ],
      "metadata": {
        "id": "MEAEiXW3uTIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenize(object):\n",
        "    def __init__(self, lang):\n",
        "        self.nlp = importlib.import_module(lang).load()\n",
        "\n",
        "    def tokenizer(self, sentence):\n",
        "        sentence = re.sub(\n",
        "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
        "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
        "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
        "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
        "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
        "        sentence = sentence.lower()\n",
        "        return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]"
      ],
      "metadata": {
        "id": "tka67FsH05Ys"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize = Tokenize('zh_core_web_sm')\n",
        "tokenize.tokenizer('你好，这里是中国。')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj4uUD4qCY7Q",
        "outputId": "e8b5dd34-d5b1-474e-d142-dde59b9e132a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['你好', '，', '这里', '是', '中国', '。']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Input Embedding"
      ],
      "metadata": {
        "id": "jRWWi86juXmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token Embedding"
      ],
      "metadata": {
        "id": "4q-d-gsquuDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedder(nn.Module):\n",
        "    # defines the layers of the model \n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Linear layer expects vectors (e.g. one-hot-encoding of words), Embedding Layer expects tokens (e.g. words index)\n",
        "        # The layer holds a Tensor of dimension: size of the vocabulary * size of each embedding vector\n",
        "        # calling Embedding(torch.LongTensor([3,4])) will return the embedding vectors corresponding to the word 3 and 4 in your vocabulary\n",
        "        self.embed = nn.Embedding(vocab_size, d_model) \n",
        "\n",
        "    # defines tensor operations that propagate input through the defined layers of the model\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)"
      ],
      "metadata": {
        "id": "wyQKCnqvDBfS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = Embedder(10, 6)\n",
        "x = embedder(torch.LongTensor([[1,2,4,5],[4,3,2,9]]))\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_SYrKz-xxvt",
        "outputId": "ccb9cadc-10d0-48e5-fa3d-c2cd3bdac7eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0213, -0.1896, -0.3202, -0.2448, -1.1269,  1.6894],\n",
              "         [-1.0196, -0.5152, -1.5480,  0.2170,  0.2013,  0.3572],\n",
              "         [ 0.4963, -1.1942, -1.1032,  0.0088,  0.4541,  1.1270],\n",
              "         [ 0.1167,  1.2405,  0.3348,  1.7332, -1.6667, -1.9068]],\n",
              "\n",
              "        [[ 0.4963, -1.1942, -1.1032,  0.0088,  0.4541,  1.1270],\n",
              "         [-1.7532,  0.7491, -0.3572, -1.2576, -0.6655,  1.8617],\n",
              "         [-1.0196, -0.5152, -1.5480,  0.2170,  0.2013,  0.3572],\n",
              "         [-2.2407, -0.2142,  0.9365,  1.0071, -0.5728,  1.0450]]],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position Encoding"
      ],
      "metadata": {
        "id": "sRs1suQYujQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len=80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # create constant positional encoding matrix\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "\n",
        "        # (max_seq_len, d_model) -> (1, max_seq_len, d_model)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Buffers won’t be returned in model.parameters(), so that the optimizer won’t have a change to update them.\n",
        "        # saved as persistent state, such as running mean in BatchNorm\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # scale input embedding to make it relatively larger, so that adding pe won't distort the original information of input embedding\n",
        "        x = x * math.sqrt(self.d_model)\n",
        "\n",
        "        # tensor.size(dim) count # of elements along given axis\n",
        "        # x : [batch?, seq_len, d_model]\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        # add positional encoding to input embedding\n",
        "        x = x + Variable(self.pe[:, :seq_len], requires_grad = False)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "LD7NeAUu6P3p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_enc = PositionalEncoder(6)\n",
        "x = pos_enc(x)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8Rr6cRvyb9m",
        "outputId": "a4e98639-a0f8-48dd-bffd-700c01fff864"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0523,  0.5355, -0.7843,  0.4004, -2.7604,  5.1382],\n",
              "         [-1.6560, -0.2630, -3.7897,  1.5315,  0.4930,  1.8749],\n",
              "         [ 2.1250, -1.9294, -2.6980,  1.0215,  1.1122,  3.7607],\n",
              "         [ 0.4270,  4.0289,  0.8267,  5.2456, -4.0827, -3.6708]],\n",
              "\n",
              "        [[ 1.2157, -1.9251, -2.7023,  1.0215,  1.1122,  3.7607],\n",
              "         [-3.4530,  2.8339, -0.8727, -2.0805, -1.6302,  5.5602],\n",
              "         [-1.5882, -0.2662, -3.7875,  1.5315,  0.4930,  1.8749],\n",
              "         [-5.3474,  0.4657,  2.3004,  3.4670, -1.4031,  3.5596]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Transformer Encoder\n",
        "\n",
        "![img](https://raw.githubusercontent.com/leox1v/dl20/b3d5b5556d1b2bd360a4abeef4fd82f056ab0301/imgs/transformer-block.svg)"
      ],
      "metadata": {
        "id": "CH9NJJcSuIi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1) Multi-Head-Attention Layer\n",
        "\n",
        "![attention](https://www.researchgate.net/publication/345482934/figure/fig2/AS:955463785013258@1604811726722/Internal-structure-of-the-Multi-Headed-Self-Attention-Mechanism-in-a-Transformer-block.png)"
      ],
      "metadata": {
        "id": "M9wvwZjGu43A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
        "    # scale qk to make qk follow std. dist. again\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(d_k)\n",
        "    print(\"in attention: q.size(): \", q.size(), \"k.T.size(): \", k.transpose(-2, -1).size(), \"score.size(): \", scores.size())\n",
        "\n",
        "    # mask: 1) in encoder & decoder, attention for padding should be zero 2) prevent decoder from peeking next word  \n",
        "    # mask==0 represent tokens which added for padding, replace 0 with -1e9 so that it become 0 after softmax\n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask==0, -1e9)\n",
        "\n",
        "    # score: (bs, heads, seq_len<# of query>, seq_len<# of score for respective query>), do softmax along the second dimension\n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "\n",
        "    # apply dropout layer on scores\n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "\n",
        "    # output: (bs, heads, seq_len<# of query>, d_model<# of dim of contextulized embedding>)\n",
        "    output = torch.matmul(scores, v)\n",
        "    return output"
      ],
      "metadata": {
        "id": "-Dd7x1vVuCej"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class MultiHeadAttention_not_preferred(nn.Module):\n",
        "#     def __init__(self, heads, d_model, dropout = 0.1):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.d_model = d_model\n",
        "#         self.d_k = d_model // heads\n",
        "#         self.h = heads\n",
        "\n",
        "#         self.q_linear = nn.Linear(d_model, d_model)\n",
        "#         self.v_linear = nn.Linear(d_model, d_model)\n",
        "#         self.k_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "#     def forward(self, q, k, v, mask=None):\n",
        "#         # batch_size\n",
        "#         bs = q.size(0)\n",
        "\n",
        "#         # perform linear operation, and split into N heads\n",
        "#         # view: reshape the data, the size -1 is inferred from other dimensions\n",
        "#         k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "#         q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "#         v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "\n",
        "#         # transpose to get dimensions bs * heads * seq_len * d_model\n",
        "#         k = k.transpose(1,2)\n",
        "#         q = q.transpose(1,2)\n",
        "#         v = v.transpose(1,2)\n",
        "\n",
        "#         # calculate attention\n",
        "#         scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
        "\n",
        "#         # concatenate heads\n",
        "#         concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
        "\n",
        "#         # dense concat using linear layer\n",
        "#         output = self.out(concat)\n",
        "\n",
        "#         return output"
      ],
      "metadata": {
        "id": "kA8P24WH3RLQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.h = heads\n",
        "\n",
        "        self.q_linear = nn.Linear(d_model, d_model * heads)\n",
        "        self.v_linear = nn.Linear(d_model, d_model * heads)\n",
        "        self.k_linear = nn.Linear(d_model, d_model * heads)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(heads * d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            q,k,v: The input embedding of shape [bs, seq_len, d_model].\n",
        "            \n",
        "        Returns:\n",
        "            Self attention tensor of shape [bs, seq_len, d_model].\n",
        "        \"\"\"\n",
        "        # batch_size\n",
        "        bs = q.size(0)\n",
        "\n",
        "        # perform linear operation, and split into N heads\n",
        "        # view: reshape the data, the size -1 is inferred from other dimensions\n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_model)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_model)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_model)\n",
        "        print(\"before attention: k.size(): \", k.size())\n",
        "\n",
        "        # transpose to get dimensions (bs, heads, seq_len, d_model)\n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "        # calculate attention to arrive at (bs, heads, seq_len, d_model)\n",
        "        scores = attention(q, k, v, self.d_model, mask, self.dropout)\n",
        "        print(\"after attention: scores.size(): \", scores.size())\n",
        "\n",
        "        # swap heads, seq_len back\n",
        "        # then fold the heads into the d_model dimension to arrive at (bs, seq_len, heads * d_model)\n",
        "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.h * self.d_model)\n",
        "        print(\"before dense, concat.size(): \", concat.size())\n",
        "\n",
        "        # concatenate heads, dense concat using linear layer to arrive at shape (bs, seq_len, d_model)\n",
        "        output = self.out(concat)\n",
        "        print(\"after dense, concat.size(): \", output.size())\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "wliYrZk5M6HM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_head = MultiHeadAttention(3, 6)\n",
        "x = multi_head(x, x, x)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKPlUYNEWMhw",
        "outputId": "25235edf-4544-45b0-bfe5-6cfb8d2d04e0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before attention: k.size():  torch.Size([2, 4, 3, 6])\n",
            "in attention: q.size():  torch.Size([2, 3, 4, 6]) k.T.size():  torch.Size([2, 3, 6, 4]) score.size():  torch.Size([2, 3, 4, 4])\n",
            "after attention: scores.size():  torch.Size([2, 3, 4, 6])\n",
            "before dense, concat.size():  torch.Size([2, 4, 18])\n",
            "after dense, concat.size():  torch.Size([2, 4, 6])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-5.1469e-01,  2.8398e-01, -6.6061e-01,  7.8064e-01, -1.2111e+00,\n",
              "           8.7032e-01],\n",
              "         [ 8.4251e-03, -1.1492e-03, -4.1300e-01,  6.6197e-01, -6.5142e-01,\n",
              "           6.0440e-02],\n",
              "         [ 5.7709e-02,  8.4160e-02, -5.5680e-01,  7.2970e-01, -7.4402e-01,\n",
              "          -9.4615e-03],\n",
              "         [-5.3445e-01, -1.8519e+00, -2.5510e-01, -4.8169e-01, -1.3785e+00,\n",
              "           1.9085e-01]],\n",
              "\n",
              "        [[-5.8775e-01, -1.7358e+00,  3.7881e-02, -5.7792e-02, -8.5012e-01,\n",
              "           7.1373e-01],\n",
              "         [-6.7315e-01, -1.4729e+00,  1.9081e-01,  2.3022e-01, -4.0392e-01,\n",
              "           6.0857e-01],\n",
              "         [-3.2952e-01, -1.4959e+00,  1.7805e-01, -3.5701e-01, -2.3952e-01,\n",
              "           9.5266e-02],\n",
              "         [-5.4317e-01, -7.2857e-01,  8.5229e-02,  5.8847e-02,  5.7733e-02,\n",
              "          -7.9071e-02]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) LayerNorm\n",
        "\n",
        "$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$"
      ],
      "metadata": {
        "id": "wZcytnVVcot4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NormLayer(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-6):\n",
        "        super().__init__()\n",
        "\n",
        "        self.size = d_model\n",
        "\n",
        "        # train two learnable parameters\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "\n",
        "        # a value added to the denominator for numerical stability\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # calculate mean over last axis: d_model\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm"
      ],
      "metadata": {
        "id": "78xcnQqWXODt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm = NormLayer(6)\n",
        "x = norm(x)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB28csSSLZB_",
        "outputId": "0e1bafc2-7393-4c69-8e60-4e26b8d6cc1a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-5.1923e-01,  4.2443e-01, -6.9165e-01,  1.0113e+00, -1.3420e+00,\n",
              "           1.1172e+00],\n",
              "         [ 1.4230e-01,  1.2108e-01, -7.9159e-01,  1.5906e+00, -1.3199e+00,\n",
              "           2.5756e-01],\n",
              "         [ 2.4953e-01,  2.9998e-01, -9.2252e-01,  1.5312e+00, -1.2796e+00,\n",
              "           1.2142e-01],\n",
              "         [ 2.4366e-01, -1.5008e+00,  6.1355e-01,  3.1351e-01, -8.7395e-01,\n",
              "           1.2040e+00]],\n",
              "\n",
              "        [[-2.0645e-01, -1.5650e+00,  5.3393e-01,  4.2071e-01, -5.1693e-01,\n",
              "           1.3337e+00],\n",
              "         [-5.5499e-01, -1.6124e+00,  5.8731e-01,  6.3941e-01, -1.9902e-01,\n",
              "           1.1397e+00],\n",
              "         [ 4.7598e-02, -1.8950e+00,  8.9299e-01,  1.8146e-03,  1.9750e-01,\n",
              "           7.5512e-01],\n",
              "         [-9.9378e-01, -1.5177e+00,  7.8200e-01,  7.0745e-01,  7.0430e-01,\n",
              "           3.1771e-01]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (3) Feed Forward Layer"
      ],
      "metadata": {
        "id": "llp-rC0LhAGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear_1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear_2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "nVhqdLoJg_hy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ff = FeedForward(6)\n",
        "x = ff(x)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TW0CagqL1G_",
        "outputId": "2c549ec2-74fe-40a7-80fe-d9868b2cee99"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.3133, -0.0840, -0.1850, -0.1372,  0.6160, -0.2736],\n",
              "         [ 0.2534,  0.0568, -0.0225,  0.1077,  0.6298, -0.1486],\n",
              "         [ 0.2518,  0.0036,  0.0836, -0.0429,  0.7081, -0.0291],\n",
              "         [ 0.2077,  0.0863, -0.1182, -0.2939,  0.4759,  0.0521]],\n",
              "\n",
              "        [[-0.0283,  0.0900, -0.1073, -0.2666,  0.3705,  0.0846],\n",
              "         [-0.1590,  0.0110, -0.1929, -0.2772,  0.4851,  0.0890],\n",
              "         [-0.1223,  0.1114, -0.1816, -0.4036,  0.3201,  0.1907],\n",
              "         [-0.4788, -0.0825, -0.2846, -0.1588,  0.4718,  0.0833]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (4) Encoder"
      ],
      "metadata": {
        "id": "hI5FTeKil04V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# one block\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = NormLayer(d_model)\n",
        "        self.norm_2 = NormLayer(d_model)\n",
        "\n",
        "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # first resdisual network\n",
        "        x_prime = self.dropout_1(self.attn(x, x, x, mask))\n",
        "        x = self.norm_1(x_prime + x)\n",
        "\n",
        "        # second residual network\n",
        "        x_prime = self.dropout_2(self.ff(x))\n",
        "        x = self.norm_2(x_prime + x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "eVPIG1scl4Qq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can then build a convenient cloning function that can generate multiple layers:\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "metadata": {
        "id": "qO8nJLjmRD9Z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module): \n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        # N: how many times to repeat the encoder block\n",
        "        self.N = N\n",
        "\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "\n",
        "        # layers: a list of encoder layers\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
        "        self.norm = NormLayer(d_model)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask)\n",
        "\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "T67YyoPPOQd6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_size = 10, d_model = 6, N = 2, heads = 3\n",
        "ec = Encoder(10, 6, 2, 3, 0.1)\n",
        "enc_outs = ec(torch.LongTensor([[1,2,4,5],[4,3,2,9]]), None)\n",
        "enc_outs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tF48E_OPicO",
        "outputId": "476ddef6-d905-4f6a-d558-f24797f10bc7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before attention: k.size():  torch.Size([2, 4, 3, 6])\n",
            "in attention: q.size():  torch.Size([2, 3, 4, 6]) k.T.size():  torch.Size([2, 3, 6, 4]) score.size():  torch.Size([2, 3, 4, 4])\n",
            "after attention: scores.size():  torch.Size([2, 3, 4, 6])\n",
            "before dense, concat.size():  torch.Size([2, 4, 18])\n",
            "after dense, concat.size():  torch.Size([2, 4, 6])\n",
            "before attention: k.size():  torch.Size([2, 4, 3, 6])\n",
            "in attention: q.size():  torch.Size([2, 3, 4, 6]) k.T.size():  torch.Size([2, 3, 6, 4]) score.size():  torch.Size([2, 3, 4, 4])\n",
            "after attention: scores.size():  torch.Size([2, 3, 4, 6])\n",
            "before dense, concat.size():  torch.Size([2, 4, 18])\n",
            "after dense, concat.size():  torch.Size([2, 4, 6])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1669, -0.8777, -0.2347, -0.8343,  1.8560, -0.0762],\n",
              "         [ 0.4490,  0.9609, -1.1416, -0.6694,  1.2113, -0.8102],\n",
              "         [ 1.4555, -0.9952,  0.1804, -1.0120,  0.8044, -0.4331],\n",
              "         [ 0.4101,  0.0764,  0.2366, -1.2998,  1.4955, -0.9187]],\n",
              "\n",
              "        [[ 0.1294, -1.4758, -0.0938, -0.6330,  1.3876,  0.6856],\n",
              "         [ 0.7848,  0.2365, -1.7956, -0.2094,  1.0286, -0.0450],\n",
              "         [ 0.1697,  0.7690, -1.3142, -0.4046,  1.4352, -0.6551],\n",
              "         [ 0.6588, -1.6030,  0.0857,  0.2879,  1.2189, -0.6483]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Transformer Decoder\n",
        "\n",
        "![img](https://raw.githubusercontent.com/leox1v/dl20/b3d5b5556d1b2bd360a4abeef4fd82f056ab0301/imgs/classifier.svg)"
      ],
      "metadata": {
        "id": "cN8yFHKET-zP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm_1 = NormLayer(d_model)\n",
        "        self.norm_2 = NormLayer(d_model)\n",
        "        self.norm_3 = NormLayer(d_model)\n",
        "        \n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "        \n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, enc_outputs, src_mask, trg_mask):\n",
        "        # first resdisual network in decoder\n",
        "        x_prime = self.dropout_1(self.attn_1(x, x, x, trg_mask))\n",
        "        x = self.norm_1(x_prime + x)\n",
        "\n",
        "        # second residual network\n",
        "        # attention(q, k, v, mask)\n",
        "        x_prime = self.dropout_2(self.attn_2(x, enc_outputs, enc_outputs, src_mask))\n",
        "        x = self.norm_2(x_prime + x)\n",
        "\n",
        "        # third residual network\n",
        "        x_prime = self.dropout_3(self.ff(x))\n",
        "        x = self.norm_3(x_prime + x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "x-sBdMWdTJo6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.N = N\n",
        "\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
        "        self.norm = NormLayer(d_model)\n",
        "\n",
        "    def forward(self, trg, enc_outputs, src_mask, trg_mask):\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, enc_outputs, src_mask, trg_mask)\n",
        "\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "BRPA52BaaoAn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_size = 10, d_model = 6, N = 2, heads = 3\n",
        "dec = Decoder(10, 6, 2, 3, 0.1)\n",
        "dec_outs = dec(torch.LongTensor([[4,3,2,9], [1,2,4,5]]), enc_outs, None, None)\n",
        "dec_outs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qydPkSBSbCOo",
        "outputId": "590cbd46-6805-48f6-cf86-0539549efaf9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before attention: k.size():  torch.Size([2, 4, 3, 6])\n",
            "in attention: q.size():  torch.Size([2, 3, 4, 6]) k.T.size():  torch.Size([2, 3, 6, 4]) score.size():  torch.Size([2, 3, 4, 4])\n",
            "after attention: scores.size():  torch.Size([2, 3, 4, 6])\n",
            "before dense, concat.size():  torch.Size([2, 4, 18])\n",
            "after dense, concat.size():  torch.Size([2, 4, 6])\n",
            "before attention: k.size():  torch.Size([2, 4, 3, 6])\n",
            "in attention: q.size():  torch.Size([2, 3, 4, 6]) k.T.size():  torch.Size([2, 3, 6, 4]) score.size():  torch.Size([2, 3, 4, 4])\n",
            "after attention: scores.size():  torch.Size([2, 3, 4, 6])\n",
            "before dense, concat.size():  torch.Size([2, 4, 18])\n",
            "after dense, concat.size():  torch.Size([2, 4, 6])\n",
            "before attention: k.size():  torch.Size([2, 4, 3, 6])\n",
            "in attention: q.size():  torch.Size([2, 3, 4, 6]) k.T.size():  torch.Size([2, 3, 6, 4]) score.size():  torch.Size([2, 3, 4, 4])\n",
            "after attention: scores.size():  torch.Size([2, 3, 4, 6])\n",
            "before dense, concat.size():  torch.Size([2, 4, 18])\n",
            "after dense, concat.size():  torch.Size([2, 4, 6])\n",
            "before attention: k.size():  torch.Size([2, 4, 3, 6])\n",
            "in attention: q.size():  torch.Size([2, 3, 4, 6]) k.T.size():  torch.Size([2, 3, 6, 4]) score.size():  torch.Size([2, 3, 4, 4])\n",
            "after attention: scores.size():  torch.Size([2, 3, 4, 6])\n",
            "before dense, concat.size():  torch.Size([2, 4, 18])\n",
            "after dense, concat.size():  torch.Size([2, 4, 6])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.7530,  0.5741,  0.0394,  1.0241, -0.8825, -1.5081],\n",
              "         [-1.4424, -0.0928,  1.0975,  1.1407, -0.6325, -0.0706],\n",
              "         [ 0.0489,  0.1028, -1.8282, -0.1047,  0.7363,  1.0449],\n",
              "         [ 0.8701,  1.0084, -0.3340,  0.7248, -1.0488, -1.2204]],\n",
              "\n",
              "        [[-0.2432,  1.6191,  0.3136,  0.3459, -1.1396, -0.8958],\n",
              "         [ 0.8243, -0.5527, -1.4423,  1.3680,  0.0494, -0.2467],\n",
              "         [ 0.9216,  0.8613, -0.0116,  0.6389, -0.9859, -1.4243],\n",
              "         [ 1.2805,  0.2038, -0.2395,  0.8916, -1.3719, -0.7644]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Complete Transformer"
      ],
      "metadata": {
        "id": "lllVPSP9bzCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):  \n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads, dropout)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "        \n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        enc_outputs = self.encoder(src, src_mask)\n",
        "        dec_output = self.decoder(trg, enc_outputs, src_mask, trg_mask)\n",
        "        output = self.out(dec_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "xdGTO6fZblbd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_size = 10, d_model = 6, N = 2, heads = 3, dropout = 0.1\n",
        "trans = Transformer(10, 10, 6, 2, 3, 0.1)\n",
        "trans(torch.LongTensor([[1,2,4,5],[4,3,2,9]]), torch.LongTensor([[4,3,2,9], [1,2,4,5]]), None, None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ4ppuY4cOIo",
        "outputId": "c069e831-8fe2-4fcf-f14b-f0d3f7e454e6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before attention: k.size():  torch.Size([2, 4, 3, 6])\n",
            "in attention: q.size():  torch.Size([2, 3, 4, 6]) k.T.size():  torch.Size([2, 3, 6, 4]) score.size():  torch.Size([2, 3, 4, 4])\n",
            "after attention: scores.size():  torch.Size([2, 3, 4, 6])\n",
            "before dense, concat.size():  torch.Size([2, 4, 18])\n",
            "after dense, concat.size():  torch.Size([2, 4, 6])\n",
            "before attention: k.size():  torch.Size([2, 4, 3, 6])\n",
            "in attention: q.size():  torch.Size([2, 3, 4, 6]) k.T.size():  torch.Size([2, 3, 6, 4]) score.size():  torch.Size([2, 3, 4, 4])\n",
            "after attention: scores.size():  torch.Size([2, 3, 4, 6])\n",
            "before dense, concat.size():  torch.Size([2, 4, 18])\n",
            "after dense, concat.size():  torch.Size([2, 4, 6])\n",
            "before attention: k.size():  torch.Size([2, 4, 3, 6])\n",
            "in attention: q.size():  torch.Size([2, 3, 4, 6]) k.T.size():  torch.Size([2, 3, 6, 4]) score.size():  torch.Size([2, 3, 4, 4])\n",
            "after attention: scores.size():  torch.Size([2, 3, 4, 6])\n",
            "before dense, concat.size():  torch.Size([2, 4, 18])\n",
            "after dense, concat.size():  torch.Size([2, 4, 6])\n",
            "before attention: k.size():  torch.Size([2, 4, 3, 6])\n",
            "in attention: q.size():  torch.Size([2, 3, 4, 6]) k.T.size():  torch.Size([2, 3, 6, 4]) score.size():  torch.Size([2, 3, 4, 4])\n",
            "after attention: scores.size():  torch.Size([2, 3, 4, 6])\n",
            "before dense, concat.size():  torch.Size([2, 4, 18])\n",
            "after dense, concat.size():  torch.Size([2, 4, 6])\n",
            "before attention: k.size():  torch.Size([2, 4, 3, 6])\n",
            "in attention: q.size():  torch.Size([2, 3, 4, 6]) k.T.size():  torch.Size([2, 3, 6, 4]) score.size():  torch.Size([2, 3, 4, 4])\n",
            "after attention: scores.size():  torch.Size([2, 3, 4, 6])\n",
            "before dense, concat.size():  torch.Size([2, 4, 18])\n",
            "after dense, concat.size():  torch.Size([2, 4, 6])\n",
            "before attention: k.size():  torch.Size([2, 4, 3, 6])\n",
            "in attention: q.size():  torch.Size([2, 3, 4, 6]) k.T.size():  torch.Size([2, 3, 6, 4]) score.size():  torch.Size([2, 3, 4, 4])\n",
            "after attention: scores.size():  torch.Size([2, 3, 4, 6])\n",
            "before dense, concat.size():  torch.Size([2, 4, 18])\n",
            "after dense, concat.size():  torch.Size([2, 4, 6])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.4984, -0.5855,  0.3687,  0.0863,  0.5848,  0.0384,  0.0076,\n",
              "           0.5324,  0.1880,  0.3326],\n",
              "         [-0.0072, -0.6426,  0.3195,  0.4552,  0.2872,  0.1384, -0.3262,\n",
              "           0.8380,  0.6184,  0.0619],\n",
              "         [-0.1415, -0.0024, -0.1499,  0.0955,  0.9216,  0.3525, -0.5664,\n",
              "           0.5567,  0.1013, -0.3077],\n",
              "         [ 0.3067, -0.7626,  0.2457, -0.0944,  0.0869,  0.0742,  0.5311,\n",
              "          -0.0109, -0.1188,  0.5765]],\n",
              "\n",
              "        [[-0.1160, -0.2672, -0.1414, -0.1742,  0.8364,  0.2723,  0.0569,\n",
              "           0.4402, -0.1929,  0.4056],\n",
              "         [ 0.0756, -0.1109, -0.0171,  0.0518,  0.9061,  0.2467, -0.4086,\n",
              "           0.5244,  0.0728, -0.1393],\n",
              "         [ 0.3312, -0.5124,  0.2151,  0.0143,  0.6618,  0.1524,  0.0024,\n",
              "           0.5341,  0.0918,  0.3563],\n",
              "         [ 1.0206, -0.7058,  0.7884,  0.3457,  0.0966, -0.2502, -0.0772,\n",
              "           0.1360,  0.4390, -0.1667]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Transformer\n",
        "\n",
        "[en blog tutorial](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec)"
      ],
      "metadata": {
        "id": "wZvzxqEiJ8hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "heads = 8\n",
        "N = 6\n",
        "src_vocab = len(EN_TEXT.vocab)\n",
        "trg_vocab = len(FR_TEXT.vocab)\n",
        "model = Transformer(src_vocab, trg_vocab, d_model, N, heads)\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "        \n",
        "# this code is very important! It initialises the parameters with a\n",
        "# range of values that stops the signal fading or getting too big.\n",
        "# See this blog for a mathematical explanation.\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "metadata": {
        "id": "2Xk1lYEgcqa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch = next(iter(train_iter))\n",
        "\n",
        "# from torchtext import data\n",
        "\n",
        "# class MyIterator(data.Iterator):\n",
        "#     def create_batches(self):\n",
        "#         if self.train:\n",
        "#             def pool(d, random_shuffler):\n",
        "#                 for p in data.batch(d, self.batch_size * 100):\n",
        "#                     p_batch = data.batch(\n",
        "#                         sorted(p, key=self.sort_key),\n",
        "#                         self.batch_size, self.batch_size_fn)\n",
        "#                     for b in random_shuffler(list(p_batch)):\n",
        "#                         yield b\n",
        "#             self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "#         else:\n",
        "#             self.batches = []\n",
        "#             for b in data.batch(self.data(), self.batch_size,\n",
        "#                                           self.batch_size_fn):\n",
        "#                 self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "\n",
        "def train_model(epochs, print_every=100):\n",
        "    model.train()\n",
        "    \n",
        "    start = time.time()\n",
        "    temp = start\n",
        "    \n",
        "    total_loss = 0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "       \n",
        "        for i, batch in enumerate(train_iter):\n",
        "            src = batch.English.transpose(0,1)\n",
        "            input_pad = EN_TEXT.vocab.stoi['<pad>']\n",
        "            # creates mask with 0s wherever there is padding in the input\n",
        "            src_mask = (src != input_pad).unsqueeze(1)\n",
        "\n",
        "            trg = batch.French.transpose(0,1)\n",
        "            trg_input = trg[:, :-1] \n",
        "            # the words we are trying to predict\n",
        "            targets = trg[:, 1:].contiguous().view(-1)\n",
        "            # the French sentence we input has all words except\n",
        "            # the last, as it is using each word to predict the next\n",
        "            # create mask as before\n",
        "            target_pad = FR_TEXT.vocab.stoi['<pad>']\n",
        "            target_msk = (trg_input != target_pad).unsqueeze(1)\n",
        "            size = trg_input.size(1) # get seq_len for matrix\n",
        "            nopeak_mask = np.triu(np.ones(1, size, size), k=1).astype('uint8')\n",
        "            nopeak_mask = Variable(torch.from_numpy(nopeak_mask) == 0)\n",
        "            trg_mask = target_msk & nopeak_mask\n",
        "\n",
        "                \n",
        "            # src_mask, trg_mask = create_masks(src, trg_input)\n",
        "            preds = model(src, trg_input, src_mask, trg_mask)\n",
        "            results = trg[:, 1:].contiguous().view(-1)\n",
        "            optim.zero_grad()\n",
        "            \n",
        "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), results, ignore_index=target_pad)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            \n",
        "            total_loss += loss.data[0]\n",
        "            if (i + 1) % print_every == 0:\n",
        "                loss_avg = total_loss / print_every\n",
        "                print(\"time = %dm, epoch %d, iter = %d, loss = %.3f, %ds per %d iters\" % \\\n",
        "                      ((time.time() - start) // 60, epoch + 1, i + 1, loss_avg, time.time() - temp, print_every))\n",
        "                total_loss = 0\n",
        "                temp = time.time()"
      ],
      "metadata": {
        "id": "cDMwEqX-KodW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Transformer\n"
      ],
      "metadata": {
        "id": "02HoZ6MkK_Vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, src, max_len = 80, custom_string=False):\n",
        "    \n",
        "    model.eval()\n",
        "    if custom_sentence == True:\n",
        "            src = tokenize_en(src)\n",
        "            sentence=\\\n",
        "            Variable(torch.LongTensor([[EN_TEXT.vocab.stoi[tok] for tok\n",
        "            in sentence]])).cuda()\n",
        "        src_mask = (src != input_pad).unsqueeze(-2)\n",
        "        e_outputs = model.encoder(src, src_mask)\n",
        "        \n",
        "        outputs = torch.zeros(max_len).type_as(src.data)\n",
        "        outputs[0] = torch.LongTensor([FR_TEXT.vocab.stoi['<sos>']])\n",
        "        \n",
        "    for i in range(1, max_len):    \n",
        "                \n",
        "            trg_mask = np.triu(np.ones((1, i, i),\n",
        "            k=1).astype('uint8')\n",
        "            trg_mask= Variable(torch.from_numpy(trg_mask) == 0).cuda()\n",
        "            \n",
        "            out = model.out(model.decoder(outputs[:i].unsqueeze(0),\n",
        "            e_outputs, src_mask, trg_mask))\n",
        "            out = F.softmax(out, dim=-1)\n",
        "            val, ix = out[:, -1].data.topk(1)\n",
        "            \n",
        "            outputs[i] = ix[0][0]\n",
        "            if ix[0][0] == FR_TEXT.vocab.stoi['<eos>']:\n",
        "                break\n",
        "    return ' '.join([FR_TEXT.vocab.itos[ix] for ix in outputs[:i]])"
      ],
      "metadata": {
        "id": "PyPTedH0K9y9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}